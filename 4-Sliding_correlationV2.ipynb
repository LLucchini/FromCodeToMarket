{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "def setup_mpl():\n",
    "    mpl.rc('font', size=15)\n",
    "    mpl.rcParams['xtick.major.pad']='12'\n",
    "    mpl.rcParams['ytick.major.pad']='12'\n",
    "    mpl.rcParams['lines.linewidth'] = 1\n",
    "    mpl.rcParams['xtick.major.width'] = 2.5\n",
    "    mpl.rcParams['ytick.major.width'] = 2.5\n",
    "    mpl.rcParams['xtick.minor.width'] = 1.5\n",
    "    mpl.rcParams['ytick.minor.width'] = 1.5\n",
    "    mpl.rcParams['xtick.major.size'] = 6\n",
    "    mpl.rcParams['ytick.major.size'] = 6\n",
    "    mpl.rcParams['xtick.minor.size'] = 3\n",
    "    mpl.rcParams['ytick.minor.size'] = 3\n",
    "    mpl.rcParams['axes.spines.right'] = True\n",
    "    mpl.rcParams['axes.spines.top'] = True\n",
    "    mpl.rcParams['xtick.minor.size'] = 1\n",
    "    mpl.rcParams['lines.linewidth']=2\n",
    "setup_mpl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute normalized&non-normalized correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation functions\n",
    "def compute_spearman_correlation(x):\n",
    "    pair1, pair2, connection_time = x.copy()\n",
    "    #Compute rolling correlation using pandas (simply look at the rank instead of the values)\n",
    "    res = df_measure[pair1].rank().rolling(WINDOW_SIZE, min_periods=MIN_WINDOW_SIZE).corr(pairwise=True, other = df_measure[pair2].rank())\n",
    "    #shift (by window size) and substract connection time\n",
    "    res.index = (res.index-connection_time)# - datetime.timedelta(int(WINDOW_SIZE))\n",
    "    return res\n",
    "\n",
    "def compute_spearman_correlation_at_t(x):\n",
    "    pair1, pair2 = x.copy()\n",
    "    #Compute rolling correlation using pandas (simply look at the rank instead of the values)\n",
    "    res = df_measure[pair1].rank().rolling(WINDOW_SIZE, min_periods=MIN_WINDOW_SIZE).corr(pairwise=True, other = df_measure[pair2].rank())\n",
    "    return res\n",
    "\n",
    "def compute_normalized_spearman_correlation(x):\n",
    "    pair1, pair2, connection_time = x.copy()\n",
    "    #Compute rolling correlation using pandas (simply look at the rank instead of the values)\n",
    "    res = df_measure[pair1].rank().rolling(WINDOW_SIZE, min_periods=MIN_WINDOW_SIZE).corr(pairwise=True, other = df_measure[pair2].rank())\n",
    "    res_norm = ((res-average_corr)/std_corr).dropna()\n",
    "\n",
    "    res_norm.index = (res_norm.index - connection_time)\n",
    "    return res_norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate over different window sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected couples: 204\n",
      "RT couples: 96920\n",
      "RTA couples: 6296\n",
      "ORT couples: 94393\n",
      "ORTA couples: 1714\n"
     ]
    }
   ],
   "source": [
    "##### \n",
    "WINDOW_SIZE_LIST = np.array([60,80,100,120,140,160,180])\n",
    "MIN_PERCENTAGE_OF_COLUMNS = 0.05\n",
    "DAY_LIM = 600\n",
    "TRANSITION_LENGTH = 150\n",
    "\n",
    "##################### LOADING DATA ###########################\n",
    "# LOAD TIMESERIES \"measure\"\n",
    "start_date = \"01-01-2014\" # \n",
    "end_date = \"30-10-2019\"   # \n",
    "\n",
    "\n",
    "for measure_serie in ['retur']:#,'price','mrcap','mrret','volum','volumret','volat','volatret']:\n",
    "    # Select the desired time period from market data\n",
    "    df_measure = pd.read_pickle('export/df_{}.pkl'.format(measure_serie))\n",
    "    tmp = df_measure[(df_measure.index<=pd.to_datetime(end_date,format='%d-%m-%Y'))&(df_measure.index>=pd.to_datetime(start_date,format='%d-%m-%Y'))].drop('year',axis=1)\n",
    "    df_measure = tmp[tmp.isna().all()[tmp.isna().all()<1].index]\n",
    "\n",
    "    #  LOAD CONNECTED COUPLES \n",
    "    edges_file_dir = 'conn_couples'         # input couples (random or real couples)\n",
    "    df_edges = pd.read_pickle(edges_file_dir+'_safe.pkl')\n",
    "    df_edges = df_edges[df_edges.coll_begins>df_measure.first_valid_index()]\n",
    "    tmp_list = df_edges.org1.append(df_edges.org2).drop_duplicates()\n",
    "    df_list = list(tmp_list[tmp_list.isin(list(df_measure.columns))])\n",
    "    df_edges = df_edges[(df_edges.coll_begins>=pd.to_datetime(start_date,format='%d-%m-%Y'))&(df_edges.coll_begins<=pd.to_datetime(end_date,format='%d-%m-%Y'))]\n",
    "    df_edges = df_edges[(df_edges.org1.isin(df_list))&(df_edges.org2.isin(df_list))].reset_index(drop=True)\n",
    "    df_edges.to_pickle(edges_file_dir+'.pkl')\n",
    "    \n",
    "    #  LOAD RANDOM T+A COUPLES (RTA)\n",
    "    edges_file_dir = 'rand_conn_time_and_age_couples'         # input couples (random or real couples)\n",
    "    rRTA_edges = pd.read_pickle(edges_file_dir+'_safe.pkl')\n",
    "    rRTA_edges = rRTA_edges[rRTA_edges.coll_begins>df_measure.first_valid_index()]\n",
    "    tmp_list = rRTA_edges.org1.append(rRTA_edges.org2).drop_duplicates()\n",
    "    df_list = list(tmp_list[tmp_list.isin(list(df_measure.columns))])\n",
    "    rRTA_edges = rRTA_edges[(rRTA_edges.coll_begins>=pd.to_datetime(start_date,format='%d-%m-%Y'))&(rRTA_edges.coll_begins<=pd.to_datetime(end_date,format='%d-%m-%Y'))]\n",
    "    rRTA_edges = rRTA_edges[(rRTA_edges.org1.isin(df_list))&(rRTA_edges.org2.isin(df_list))].reset_index(drop=True)\n",
    "    rRTA_edges = rRTA_edges.drop_duplicates(['org1','org2','coll_begins']).reset_index(drop=True)\n",
    "    rRTA_edges = rRTA_edges.merge(rRTA_edges.rename(columns={'org1':'org2','org2':'org1'}),on=['org1','org2','coll_begins'], indicator=True,how='left').query('_merge==\"left_only\"').reset_index(drop=True)\n",
    "    rRTA_edges.to_pickle(edges_file_dir+'.pkl')\n",
    "    \n",
    "    #  LOAD RANDOM T+A COUPLES (RT)\n",
    "    edges_file_dir = 'rand_conn_time_couples'\n",
    "    rRT_edges = pd.read_pickle(edges_file_dir+'_safe.pkl')\n",
    "    rRT_edges = rRT_edges[rRT_edges.coll_begins>df_measure.first_valid_index()]\n",
    "    tmp_list = rRT_edges.org1.append(rRT_edges.org2).drop_duplicates()\n",
    "    df_list = list(tmp_list[tmp_list.isin(list(df_measure.columns))])\n",
    "    rRT_edges = rRT_edges[(rRT_edges.coll_begins>=pd.to_datetime(start_date,format='%d-%m-%Y'))&(rRT_edges.coll_begins<=pd.to_datetime(end_date,format='%d-%m-%Y'))]\n",
    "    rRT_edges = rRT_edges[(rRT_edges.org1.isin(df_list))&(rRT_edges.org2.isin(df_list))].reset_index(drop=True)\n",
    "    rRT_edges = rRT_edges.drop_duplicates(['org1','org2','coll_begins']).reset_index(drop=True)\n",
    "    rRT_edges = rRT_edges.merge(rRT_edges.rename(columns={'org1':'org2','org2':'org1'}),on=['org1','org2','coll_begins'], indicator=True,how='left').query('_merge==\"left_only\"').reset_index(drop=True)\n",
    "    rRT_edges.to_pickle(edges_file_dir+'.pkl')\n",
    "\n",
    "    #  LOAD ONE RANDOM T COUPLES (ORT)\n",
    "    edges_file_dir = 'rand_conn_time_couples'\n",
    "    rORT_edges = pd.read_pickle(edges_file_dir+'_safe.pkl')\n",
    "    rORT1 = rORT_edges.copy()\n",
    "    rORT1['org1'] = rORT1.original_org1\n",
    "    rORT2 = rORT_edges.copy()\n",
    "    rORT2['org2'] = rORT2.original_org2\n",
    "    rORT_edges = rORT1.append(rORT2).reset_index(drop=True)\n",
    "    rORT_edges = rORT_edges[rORT_edges.coll_begins>df_measure.first_valid_index()]\n",
    "    tmp_list = rORT_edges.org1.append(rORT_edges.org2).drop_duplicates()\n",
    "    df_list = list(tmp_list[tmp_list.isin(list(df_measure.columns))])\n",
    "    rORT_edges = rORT_edges[(rORT_edges.coll_begins>=pd.to_datetime(start_date,format='%d-%m-%Y'))&(rORT_edges.coll_begins<=pd.to_datetime(end_date,format='%d-%m-%Y'))]\n",
    "    rORT_edges = rORT_edges[(rORT_edges.org1.isin(df_list))&(rORT_edges.org2.isin(df_list))].reset_index(drop=True)\n",
    "    rORT_edges = rORT_edges.drop_duplicates(['org1','org2','coll_begins']).reset_index(drop=True)\n",
    "    rORT_edges = rORT_edges.merge(rORT_edges.rename(columns={'org1':'org2','org2':'org1'}),on=['org1','org2','coll_begins'], indicator=True,how='left').query('_merge==\"left_only\"').reset_index(drop=True)\n",
    "    rORT_edges = rORT_edges.sample(len(rRT_edges),replace=True).reset_index(drop=True)\n",
    "\n",
    "    #  LOAD ONE RANDOM T+A COUPLES (ORTA)\n",
    "    edges_file_dir = 'rand_conn_time_and_age_couples'\n",
    "    rORTA_edges = pd.read_pickle(edges_file_dir+'_safe.pkl')\n",
    "    rORTA1 = rORTA_edges.copy()\n",
    "    rORTA1['org1'] = rORTA1.original_org1\n",
    "    rORTA2 = rORTA_edges.copy()\n",
    "    rORTA2['org2'] = rORTA2.original_org2\n",
    "    rORTA_edges = rORTA1.append(rORTA2).reset_index(drop=True)\n",
    "    rORTA_edges = rORTA_edges[rORTA_edges.coll_begins>df_measure.first_valid_index()]\n",
    "    tmp_list = rORTA_edges.org1.append(rORTA_edges.org2).drop_duplicates()\n",
    "    df_list = list(tmp_list[tmp_list.isin(list(df_measure.columns))])\n",
    "    rORTA_edges = rORTA_edges[(rORTA_edges.coll_begins>=pd.to_datetime(start_date,format='%d-%m-%Y'))&(rORTA_edges.coll_begins<=pd.to_datetime(end_date,format='%d-%m-%Y'))]\n",
    "    rORTA_edges = rORTA_edges[(rORTA_edges.org1.isin(df_list))&(rORTA_edges.org2.isin(df_list))].reset_index(drop=True)\n",
    "    rORTA_edges = rORTA_edges.drop_duplicates(['org1','org2','coll_begins']).reset_index(drop=True)\n",
    "    rORTA_edges = rORTA_edges.merge(rORTA_edges.rename(columns={'org1':'org2','org2':'org1'}),on=['org1','org2','coll_begins'], indicator=True,how='left').query('_merge==\"left_only\"').reset_index(drop=True)\n",
    "    ##################### END LOADING ###########################\n",
    "\n",
    "    # Connected Couples\n",
    "    pairs = df_edges[['org1','org2','coll_begins']]\n",
    "    pairsRTA = rRTA_edges[['org1','org2','coll_begins']]\n",
    "    pairsRT = rRT_edges[['org1','org2','coll_begins']]\n",
    "    pairsORT = rORT_edges[['org1','org2','coll_begins']]\n",
    "    pairsORTA = rORTA_edges[['org1','org2','coll_begins']]\n",
    "\n",
    "    # Remove connected couples from random couples\n",
    "    pairsRT = pairsRT.merge(pairs[['org1','org2']],on=['org1','org2'], indicator=True,how='left').query('_merge==\"left_only\"').drop('_merge', axis=1).reset_index(drop=True)\n",
    "    pairsRT = pairsRT.rename(columns={'org1':'org2','org2':'org1'}).merge(pairs[['org1','org2']],on=['org1','org2'], indicator=True,how='left').query('_merge==\"left_only\"').drop('_merge', axis=1).reset_index(drop=True)\n",
    "\n",
    "    pairsRTA = pairsRTA.merge(pairs[['org1','org2']],on=['org1','org2'], indicator=True,how='left').query('_merge==\"left_only\"').drop('_merge', axis=1).reset_index(drop=True)\n",
    "    pairsRTA = pairsRTA.rename(columns={'org1':'org2','org2':'org1'}).merge(pairs[['org1','org2']],on=['org1','org2'], indicator=True,how='left').query('_merge==\"left_only\"').drop('_merge', axis=1).reset_index(drop=True)\n",
    "\n",
    "    pairsORT = pairsORT.merge(pairs[['org1','org2']],on=['org1','org2'], indicator=True,how='left').query('_merge==\"left_only\"').drop('_merge', axis=1).reset_index(drop=True)\n",
    "    pairsORT = pairsORT.rename(columns={'org1':'org2','org2':'org1'}).merge(pairs[['org1','org2']],on=['org1','org2'], indicator=True,how='left').query('_merge==\"left_only\"').drop('_merge', axis=1).reset_index(drop=True)\n",
    "    pairsORTA = pairsORTA.merge(pairs[['org1','org2']],on=['org1','org2'], indicator=True,how='left').query('_merge==\"left_only\"').drop('_merge', axis=1).reset_index(drop=True)\n",
    "    pairsORTA = pairsORTA.rename(columns={'org1':'org2','org2':'org1'}).merge(pairs[['org1','org2']],on=['org1','org2'], indicator=True,how='left').query('_merge==\"left_only\"').drop('_merge', axis=1).reset_index(drop=True)\n",
    "\n",
    "    # Ecology representatives Random Couples\n",
    "    cryptos = df_measure.dropna(how='all',axis=1).columns\n",
    "    crypto_pairs = pd.DataFrame(itertools.combinations(cryptos,2))\n",
    "    # ## Remove connected couples from crypto_pairs\n",
    "    crypto_pairs = crypto_pairs.rename(columns={0:'org1',1:'org2'}).merge(pairs[['org1','org2']],on=['org1','org2'], indicator=True,how='left').query('_merge==\"left_only\"').drop('_merge', axis=1).reset_index(drop=True)\n",
    "    crypto_pairs = crypto_pairs.rename(columns={0:'org2',1:'org1'}).merge(pairs[['org1','org2']],on=['org1','org2'], indicator=True,how='left').query('_merge==\"left_only\"').drop('_merge', axis=1).reset_index(drop=True)\n",
    "\n",
    "    print('Connected couples: {}'.format(len(pairs)))\n",
    "    print('RT couples: {}'.format(len(pairsRT)))\n",
    "    print('RTA couples: {}'.format(len(pairsRTA)))\n",
    "    print('ORT couples: {}'.format(len(pairsORT)))\n",
    "    print('ORTA couples: {}'.format(len(pairsORTA)))\n",
    "\n",
    "    for WINDOW_SIZE in WINDOW_SIZE_LIST:\n",
    "        MIN_WINDOW_SIZE = int(0.95*WINDOW_SIZE)\n",
    "         ## SPEARMAN CORRELATION\n",
    "        r = pairs.apply(compute_spearman_correlation,axis = 1).T\n",
    "        r.replace(np.inf,np.NaN,inplace=True)\n",
    "        r.replace(-np.inf,np.NaN,inplace=True)\n",
    "        r = r.dropna(axis=1,how='all')\n",
    "        # remove indexes with low statistics\n",
    "        threshold = r.count(axis=1)/len(r.columns)\n",
    "        threshold = threshold[(threshold>MIN_PERCENTAGE_OF_COLUMNS)&(threshold.index>=f'-{DAY_LIM} days')&(threshold.index<=f'{DAY_LIM+TRANSITION_LENGTH} days')]\n",
    "        r = r.loc[threshold.index]\n",
    "\n",
    "        rmean = r.mean(axis = 1)\n",
    "        rstd = r.sem(axis = 1)\n",
    "\n",
    "    ## RTA couples correlation\n",
    "        rRTA = pairsRTA.apply(compute_spearman_correlation,axis = 1).T\n",
    "        rRTA.replace(np.inf,np.NaN,inplace=True)\n",
    "        rRTA.replace(-np.inf,np.NaN,inplace=True)\n",
    "        rRTA = rRTA.loc[threshold.index]\n",
    "        rmeanRTA = rRTA.mean(axis = 1)\n",
    "        rstdRTA = rRTA.sem(axis = 1)\n",
    "    ## ORTA couples correlation\n",
    "        rORTA = pairsORTA.apply(compute_spearman_correlation,axis = 1).T\n",
    "        rORTA.replace(np.inf,np.NaN,inplace=True)\n",
    "        rORTA.replace(-np.inf,np.NaN,inplace=True)\n",
    "        rORTA = rORTA.loc[threshold.index]\n",
    "        rmeanORTA = rORTA.mean(axis = 1)\n",
    "        rstdORTA = rORTA.sem(axis = 1)\n",
    "\n",
    "    ## RT couples correlation\n",
    "        rRT = pairsRT.apply(compute_spearman_correlation,axis = 1).T\n",
    "        rRT.replace(np.inf,np.NaN,inplace=True)\n",
    "        rRT.replace(-np.inf,np.NaN,inplace=True)\n",
    "        rRT = rRT.loc[threshold.index]\n",
    "        rmeanRT = rRT.mean(axis = 1)\n",
    "        rstdRT = rRT.sem(axis = 1)\n",
    "    ## ORT couples correlation\n",
    "        rORT = pairsORT.apply(compute_spearman_correlation,axis = 1).T\n",
    "        rORT.replace(np.inf,np.NaN,inplace=True)\n",
    "        rORT.replace(-np.inf,np.NaN,inplace=True)\n",
    "        rORT = rORT.loc[threshold.index]\n",
    "        rmeanORT = rORT.mean(axis = 1)\n",
    "        rstdORT = rORT.sem(axis = 1)\n",
    "\n",
    "    ### End of correlation ###        \n",
    "    # ##### NORMALIZED CORRELATION #####\n",
    "        #TAKE ONLY A RANDOM SAMPLE OF CRYPTOS AND COMPUTE THE DAILY CORRELATION\n",
    "        t = time.time()\n",
    "        rt = crypto_pairs.apply(compute_spearman_correlation_at_t,axis = 1)\n",
    "        print(time.time() -t)\n",
    "        rt.replace(np.inf,np.NaN,inplace=True)\n",
    "        rt.replace(-np.inf,np.NaN,inplace=True)\n",
    "    ### comment the following 2 lines if you want to remove non-active cryptos from normalization \n",
    "    ### (uncomment lines in \"compute_normalized_spearman_correlation\")\n",
    "        average_corr = rt.mean()\n",
    "        std_corr = rt.std()\n",
    "\n",
    "        #Compute the normalized daily correlation\n",
    "        rp = pairs.apply(compute_normalized_spearman_correlation,axis = 1).T\n",
    "        rp.replace(np.inf,np.NaN,inplace=True)\n",
    "        rp.replace(-np.inf,np.NaN,inplace=True)\n",
    "        rp = rp.dropna(axis=1,how='all')\n",
    "        # remove indexes with low statistics\n",
    "        thresholdp = rp.count(axis=1)/len(rp.columns)\n",
    "        thresholdp = thresholdp[(thresholdp>MIN_PERCENTAGE_OF_COLUMNS)&(thresholdp.index>=f'-{DAY_LIM} days')&(thresholdp.index<=f'{DAY_LIM+TRANSITION_LENGTH} days')]\n",
    "        rp = rp.loc[thresholdp.index]\n",
    "        rpmean = rp.mean(axis = 1)\n",
    "        rpstd = rp.sem(axis = 1)\n",
    "\n",
    "    ## RTA stdandardized couples correlation\n",
    "        rpRTA = pairsRTA.apply(compute_normalized_spearman_correlation,axis = 1).T\n",
    "        rpRTA.replace(np.inf,np.NaN,inplace=True)\n",
    "        rpRTA.replace(-np.inf,np.NaN,inplace=True)\n",
    "        rpRTA = rpRTA.loc[thresholdp.index]\n",
    "        rpmeanRTA = rpRTA.mean(axis = 1)\n",
    "        rpstdRTA = rpRTA.sem(axis = 1)\n",
    "    ## ORTA stdandardized couples correlation\n",
    "        rpORTA = pairsORTA.apply(compute_normalized_spearman_correlation,axis = 1).T\n",
    "        rpORTA.replace(np.inf,np.NaN,inplace=True)\n",
    "        rpORTA.replace(-np.inf,np.NaN,inplace=True)\n",
    "        rpORTA = rpORTA.loc[thresholdp.index]\n",
    "        rpmeanORTA = rpORTA.mean(axis = 1)\n",
    "        rpstdORTA = rpORTA.sem(axis = 1)\n",
    "\n",
    "    ## RT stdandardized couples correlation\n",
    "        rpRT = pairsRT.apply(compute_normalized_spearman_correlation,axis = 1).T\n",
    "        rpRT.replace(np.inf,np.NaN,inplace=True)\n",
    "        rpRT.replace(-np.inf,np.NaN,inplace=True)\n",
    "        rpRT = rpRT.loc[thresholdp.index]\n",
    "        rpmeanRT = rpRT.mean(axis = 1)\n",
    "        rpstdRT = rpRT.sem(axis = 1)\n",
    "    ## ORT couples correlation\n",
    "        rpORT = pairsORT.apply(compute_normalized_spearman_correlation,axis = 1).T\n",
    "        rpORT.replace(np.inf,np.NaN,inplace=True)\n",
    "        rpORT.replace(-np.inf,np.NaN,inplace=True)\n",
    "        rpORT = rpORT.loc[threshold.index]\n",
    "        rpmeanORT = rpORT.mean(axis = 1)\n",
    "        rpstdORT = rpORT.sem(axis = 1)\n",
    "\n",
    "\n",
    "        r.to_pickle(f'export/robustness/r_{measure_serie}_win{WINDOW_SIZE}.pkl')\n",
    "        rRTA.to_pickle(f'export/robustness/rRTA_{measure_serie}_win{WINDOW_SIZE}.pkl')\n",
    "        rORTA.to_pickle(f'export/robustness/rORTA_{measure_serie}_win{WINDOW_SIZE}.pkl')\n",
    "        rRT.to_pickle(f'export/robustness/rRT_{measure_serie}_win{WINDOW_SIZE}.pkl')\n",
    "        rORT.to_pickle(f'export/robustness/rORT_{measure_serie}_win{WINDOW_SIZE}.pkl')\n",
    "\n",
    "        rp.to_pickle(f'export/robustness/rp_{measure_serie}_win{WINDOW_SIZE}.pkl')\n",
    "        rpRTA.to_pickle(f'export/robustness/rpRTA_{measure_serie}_win{WINDOW_SIZE}.pkl')\n",
    "        rpORTA.to_pickle(f'export/robustness/rpORTA_{measure_serie}_win{WINDOW_SIZE}.pkl')\n",
    "        rpRT.to_pickle(f'export/robustness/rpRT_{measure_serie}_win{WINDOW_SIZE}.pkl')\n",
    "        rpORT.to_pickle(f'export/robustness/rpORT_{measure_serie}_win{WINDOW_SIZE}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Bootstrapping \n",
    "sample_number = 10000\n",
    "measures = ['retur']#,'mrcap','price','mrret','volum','volat','volumret']\n",
    "windows = [60,80,100,120,140,160,180]\n",
    "def boot_sem(x):\n",
    "    days = x.name\n",
    "    x_val = x.dropna()\n",
    "    tmp_mean = np.zeros(sample_number)\n",
    "    for i in range(sample_number):\n",
    "        tmp_sample = x_val.sample(size_serie.loc[days],replace=True)\n",
    "        tmp_mean[i] = tmp_sample.mean()\n",
    "    return [np.std(tmp_mean),np.mean(tmp_mean)]\n",
    "\n",
    "def boot_seM(x):\n",
    "    days = x.name\n",
    "    x_val = x.dropna()\n",
    "    tmp_mean = np.zeros(sample_number)\n",
    "    for i in range(sample_number):\n",
    "        tmp_sample = x_val.sample(size_serie.loc[days],replace=True)\n",
    "        tmp_mean[i] = tmp_sample.median()\n",
    "    return [np.std(tmp_mean),np.mean(tmp_mean),np.median(tmp_mean),np.quantile(tmp_mean,0.025),np.quantile(tmp_mean,0.975)]\n",
    "\n",
    "### ROBUSTNESS TEST -> Median instead of mean (only for one window) \n",
    "sample_number = 10000\n",
    "measure = 'retur'\n",
    "window = 120\n",
    "\n",
    "df_tmp = pd.read_pickle(f'export/robustness/rp_{measure}_win{window}.pkl')\n",
    "df_tmp_time_rand = pd.read_pickle(f'export/robustness/rpRT_{measure}_win{window}.pkl')\n",
    "\n",
    "dfr = pd.DataFrame(index=df_tmp.index)\n",
    "dforta = pd.DataFrame(index=df_tmp.index)\n",
    "dfrta = pd.DataFrame(index=df_tmp.index)\n",
    "dfrt = pd.DataFrame(index=df_tmp.index)\n",
    "dfr['avg'] = df_tmp.mean(axis=1)\n",
    "dfr['sem'] = df_tmp.sem(axis=1)\n",
    "dfr['std'] = df_tmp.std(axis=1)\n",
    "size_serie = df_tmp.count(axis=1)\n",
    "dfr[['sem_N','avg_N']] = df_tmp.apply(lambda x: boot_sem(x),axis=1,result_type='expand')\n",
    "dfr[['sem_M','avg_M','med_M','left_quant_M','right_quant_M']] = df_tmp.apply(lambda x: boot_seM(x),axis=1,result_type='expand')\n",
    "dfr['active_cryptos'] = df_tmp.count(axis=1)\n",
    "\n",
    "dfrt['avg'] = df_tmp_time_rand.mean(axis=1)\n",
    "dfrt['sem'] = df_tmp_time_rand.sem(axis=1)\n",
    "dfrt['std'] = df_tmp_time_rand.std(axis=1)\n",
    "size_serie = df_tmp.count(axis=1)\n",
    "dfrt[['sem_N','avg_N']] = df_tmp_time_rand.apply(lambda x: boot_sem(x),axis=1,result_type='expand')\n",
    "dfrt[['sem_M','avg_M','med_M','left_quant_M','right_quant_M']] = df_tmp_time_rand.apply(lambda x: boot_seM(x),axis=1,result_type='expand')\n",
    "dfrt['active_cryptos'] = df_tmp_time_rand.count(axis=1)\n",
    "\n",
    "dfr.to_pickle(f'export/sCC_summary_{measure}_{sample_number}_win{window}_M.pkl')\n",
    "dfrt.to_pickle(f'export/sRT_summary_{measure}_{sample_number}_win{window}_M.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For long run get notified when computations finish\n",
    "# from scriptUltraNotifier_bot import *\n",
    "# load_logger('Notebook 4 robustness first ended!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
