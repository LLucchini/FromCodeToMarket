{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the edges: collaborative cryptocurrencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries...\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patheffects as pe\n",
    "import sys\n",
    "import imp\n",
    "%matplotlib inline\n",
    "\n",
    "from notebooks_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206\r"
     ]
    }
   ],
   "source": [
    "# ###### 0.0 load github push/accepted-pull data\n",
    "tmp = pd.read_pickle('export/git_data_pushpull_v4(vol1e5).pkl')\n",
    "tmp1 =tmp.copy()\n",
    "tmp1 = tmp1[tmp1.accepted]\n",
    "tmp1 = tmp1[(tmp1.actor_login.notna())]\n",
    "tmp1 = tmp1.reset_index(drop=True)\n",
    "\n",
    "###### Generates EDGES dataframe\n",
    "edges_list = []\n",
    "group_tmp = tmp1.groupby('actor_login')\n",
    "for actor in tmp1.drop_duplicates('actor_login').actor_login:\n",
    "    tmp_g = group_tmp.get_group(actor)\n",
    "    if len(tmp_g)>1:\n",
    "        tmp_g = tmp_g.groupby(['sym', 'date']).count().reset_index()\n",
    "        tmp_count = tmp_g.groupby(['sym']).count().reset_index().sort_values('sym')[['sym','id']]\n",
    "        tmp_g = tmp_g.sort_values('date').drop_duplicates('sym').sort_values('sym')\n",
    "        list1 = list(tmp_g.sym)\n",
    "        list2 = list(tmp_g.sym)\n",
    "        tmp_g = tmp_g.set_index('sym')\n",
    "        tmp_count = tmp_count.set_index('sym')\n",
    "        for org1 in list1:\n",
    "            list2.remove(org1)\n",
    "            for org2 in list2:\n",
    "                if org1 != org2:\n",
    "                    edges_list.append([org1,org2,tmp_count.loc[org1]['id'],tmp_count.loc[org2]['id'],\n",
    "                                       actor,tmp_g.loc[[org1,org2]]['date'].max(),\n",
    "                                       tmp_g.loc[[org1]]['date'].max(),tmp_g.loc[[org2]]['date'].max()])\n",
    "df_edges = pd.DataFrame({'org1':[edges_list[i][0] for i in range(len(edges_list))],\n",
    "                         'org2':[edges_list[i][1] for i in range(len(edges_list))],\n",
    "                         'events1':[edges_list[i][2] for i in range(len(edges_list))],\n",
    "                         'events2':[edges_list[i][3] for i in range(len(edges_list))],\n",
    "                         'actors':[edges_list[i][4] for i in range(len(edges_list))],\n",
    "                         'collaboration_begins':[edges_list[i][5] for i in range(len(edges_list))],\n",
    "                         'org1_begins':[edges_list[i][6] for i in range(len(edges_list))],\n",
    "                         'org2_begins':[edges_list[i][7] for i in range(len(edges_list))]})\n",
    "actors_per_sym = tmp1.groupby(['sym','actor_login']).count().reset_index().groupby('sym').count()[['actor_login']].reset_index()\n",
    "\n",
    "# Duplicate frame to count editors, actions, and events\n",
    "df_edges_tmp1 = df_edges.groupby(['org1','org2']).count().reset_index()[['actors','org1','org2']].reset_index(drop=True)\n",
    "df_edges_tmp2 = df_edges.groupby(['org1','org2']).sum().reset_index()[['org1','org2','events1','events2']].reset_index(drop=True)\n",
    "df_edges_collaboration = df_edges.sort_values('collaboration_begins').copy()\n",
    "df_edges_collaboration = df_edges_collaboration.drop_duplicates(['org1','org2'])[['org1','org2','collaboration_begins','org1_begins','org2_begins']].reset_index(drop=True)\n",
    "### Sort the dataframes\n",
    "df_edges.sort_values(['org1','org2','collaboration_begins'], inplace=True)\n",
    "df_edges_tmp1.sort_values(['org1','org2'], inplace=True)\n",
    "df_edges_tmp2.sort_values(['org1','org2'], inplace=True)\n",
    "df_edges_collaboration.sort_values(['org1','org2'], inplace=True)\n",
    "### Merge the 3 dataframe \n",
    "df_edges = df_edges.drop_duplicates(['org1','org2'],keep='first').reset_index(drop=True)\n",
    "df_edges['index'] = df_edges_tmp1['actors'].reset_index(drop=True)\n",
    "df_edges['coll_begins'] = df_edges_collaboration['collaboration_begins'].reset_index(drop=True)\n",
    "df_edges['org1_begins'] = df_edges_collaboration['org1_begins'].reset_index(drop=True)\n",
    "df_edges['org2_begins'] = df_edges_collaboration['org2_begins'].reset_index(drop=True)\n",
    "\n",
    "df_edges['weight_events'] = (df_edges['events1']+df_edges['events2'])/2\n",
    "df_edges.rename(columns={'index':'weights'}, inplace=True)\n",
    "df_edges['weights'] = df_edges['weights']/df_edges['weights'].sum()\n",
    "df_nodes = tmp.drop_duplicates(['actor_login','sym']).groupby('sym').count().sort_index().reset_index()[['sym','id']]\n",
    "df_nodes = df_nodes.rename(columns={'sym':'org1','id':'weights'})\n",
    "df_nodes.reset_index(inplace=True)\n",
    "df_edges = df_nodes[['index','org1']].merge(df_edges, on='org1', how='inner')\\\n",
    "                                     .rename(columns={'index':'i1'})\n",
    "df_edges = df_edges.merge(df_nodes[['index','org1']], left_on='org2', \n",
    "                                            right_on='org1', how='inner', suffixes=('','_y'))\\\n",
    "                                     .rename(columns={'index':'i2'})\n",
    "df_edges = df_edges.sort_values(['i1','org1']).drop('org1_y', axis=1)\n",
    "df_edges.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "crypto_first_action = tmp1.sort_values('date').drop_duplicates('sym')[['sym','date']].reset_index(drop=True).rename(columns={'sym':'org','date':'first_action'})\n",
    "df_edges = df_edges.merge(crypto_first_action, left_on='org1',right_on='org',how='left',suffixes=('','_org1'))\n",
    "df_edges = df_edges.merge(crypto_first_action, left_on='org2',right_on='org',how='left',suffixes=('','_org2'))\n",
    "## First_action_org1 is the first event date made by connecting editor on crypto1\n",
    "df_edges = df_edges.rename(columns={'first_action':'first_action_org1'}).drop(['collaboration_begins','org','org_org2'],axis=1)\n",
    "\n",
    "tmp = pd.read_pickle('export/git_data_v4(vol1e5).pkl')\n",
    "tmp = tmp.sort_values('created_at')\n",
    "coll_action_latter = []\n",
    "org_latter = []\n",
    "### first action (not necessarily push/pull) on second crypto\n",
    "### which one is older (from a market perspective)\n",
    "coll_action_former = []\n",
    "org_former = []\n",
    "for i,couple in df_edges.iterrows():\n",
    "    tmp2 = tmp[(tmp.actor_login == couple.actors)&((tmp.sym==couple.org1)|(tmp.sym==couple.org2))]\n",
    "    tmp2 = tmp2.drop_duplicates('sym')\n",
    "    org_former.append(tmp2.iloc[0]['sym'])\n",
    "    org_latter.append(tmp2.iloc[1]['sym'])\n",
    "    coll_action_former.append(tmp2.iloc[0]['date'])\n",
    "    coll_action_latter.append(tmp2.iloc[1]['date'])\n",
    "    print(i,end='\\r')\n",
    "df_edges['org_former'] = org_former\n",
    "df_edges['org_latter'] = org_latter\n",
    "df_edges['coll_action_former'] = coll_action_former\n",
    "df_edges['coll_action_latter'] = coll_action_latter\n",
    "\n",
    "tmp = pd.read_pickle('export/git_data_pushpull_v4(vol1e5).pkl')\n",
    "tmp = tmp.sort_values('created_at')\n",
    "coll_action_latter = []\n",
    "org_latter = []\n",
    "\n",
    "### first push/pull on second crypto\n",
    "coll_action_former = []\n",
    "org_former = []\n",
    "for i,couple in df_edges.iterrows():\n",
    "    tmp2 = tmp[(tmp.actor_login == couple.actors)&((tmp.sym==couple.org1)|(tmp.sym==couple.org2))]\n",
    "    tmp2 = tmp2.drop_duplicates('sym')\n",
    "    org_former.append(tmp2.iloc[0]['sym'])\n",
    "    org_latter.append(tmp2.iloc[1]['sym'])\n",
    "    print(i,end='\\r')\n",
    "df_edges['org_formerPP'] = org_former\n",
    "df_edges['org_latterPP'] = org_latter\n",
    "\n",
    "df_edges.to_pickle('export/df_edges_all_length_Series_tmp.pkl')\n",
    "df_nodes.to_pickle('export/df_nodes.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 %\t\t\r"
     ]
    }
   ],
   "source": [
    "### Processing CrypoCompare data\n",
    "# latest time at which data were downloaded\n",
    "now = '20191101' ### insert present day\n",
    "##### 0.1 preparing the yearly correlation between cryptos\n",
    "##### 0.1.1- Process data market timeseries\n",
    "### a crypto is considered to be active during a specific year if there are less than \"threshold\" Nan in its serie\n",
    "threshold = 10\n",
    "### 3 Options: first_action = based on first git-action of a crypto\n",
    "###            org_begins   = based on first edit by connecting user\n",
    "###            coin_age     = based on market time serie length\n",
    "### Columns with which we want to sort the org1=PASSIVE and org2 =ACTIVE\n",
    "### coll_begins  = first to edit is PASSIVE, second (at coll_begin) ACTIVE; \n",
    "### coin_age     = longhest serie is PASSIVE, shortest is ACTIVE\n",
    "### first_action = oldest crypto with git-event is PASSIVE, the youngest is ACTIVE\n",
    "\n",
    "# let pandas treat infinities as NaN\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "### Load data... \n",
    "### consdider only time series for which we have market data (uncomment if wanted)\n",
    "series = pd.read_pickle(f't_series_{now}.pkl')\n",
    "series['time'] = pd.to_datetime(series.date)\n",
    "series.rename(columns={'Close**':'price'}, inplace=True)\n",
    "series.rename(columns={'Market Cap':'market cap'}, inplace=True)\n",
    "series.rename(columns={'Volume':'volume'}, inplace=True)\n",
    "series.sort_values('time',inplace=True)\n",
    "series.loc[series.Low<0,'Low'] = np.nan\n",
    "series.loc[series.High<0,'High'] = np.nan\n",
    "\n",
    "# Evaluate the volatility per day as (max-min)/(max+min) del prezzo nella giornata\n",
    "series['volatility'] = np.log(series['High']/series['Low'])\n",
    "series.reset_index(drop=True,inplace=True)\n",
    "series_keep = series.groupby('sym').count().reset_index()\n",
    "coin_grouped_series = series.groupby('sym')\n",
    "\n",
    "# Initialize the dataframe that will contain all the crypto timeseries\n",
    "df_price = series.drop_duplicates('date')[['date']].reset_index(drop=True).rename(columns={'date':'time'})\n",
    "df_mrcap = df_price.copy()\n",
    "df_volum = df_price.copy()\n",
    "df_volat = df_price.copy()\n",
    "# Initialize the stable index to merge with\n",
    "df_price_index = df_price.copy()\n",
    "df_mrcap_index = df_price.copy()\n",
    "df_volum_index = df_price.copy()\n",
    "df_volat_index = df_price.copy()\n",
    "\n",
    "# Cicle over all the grouped cryptocurrency timeseries\n",
    "i=0\n",
    "for group1 in coin_grouped_series:\n",
    "    df_price[group1[0]] = df_price_index.merge(coin_grouped_series.get_group(group1[0])[['price','time']],left_on='time',right_on='time',how='left')['price']\n",
    "    df_mrcap[group1[0]] = df_mrcap_index.merge(coin_grouped_series.get_group(group1[0])[['market cap','time']],left_on='time',right_on='time', how='left')['market cap']\n",
    "    df_volum[group1[0]] = df_volum_index.merge(coin_grouped_series.get_group(group1[0])[['volume','time']],left_on='time',right_on='time', how='left')['volume']\n",
    "    df_volat[group1[0]] = df_volat_index.merge(coin_grouped_series.get_group(group1[0])[['volatility','time']],left_on='time',right_on='time', how='left')['volatility']\n",
    "    i+=1\n",
    "    print(round(i/len(coin_grouped_series)*100,2),'%\\t\\t',end='\\r')\n",
    "\n",
    "###### Filtering cryptos with volume > 1e5 (already removed in notebook 1)\n",
    "tmp_vol_filter = (df_volum.mean(axis=0)>1e5)\n",
    "df_volum = df_volum.set_index('time')\n",
    "df_volum = df_volum[[i for i in df_volum.columns if tmp_vol_filter.loc[i]==True]]\n",
    "df_price = df_price.set_index('time')[df_volum.columns]\n",
    "df_mrcap = df_mrcap.set_index('time')[df_volum.columns]\n",
    "df_volat = df_volat.set_index('time')[df_volum.columns]\n",
    "\n",
    "\n",
    "df_price['year'] = df_price.index.year\n",
    "df_mrcap['year'] = df_mrcap.index.year\n",
    "df_volum['year'] = df_volum.index.year\n",
    "df_volat['year'] = df_volat.index.year\n",
    "df_retur = df_price.copy()\n",
    "df_mrret = df_mrcap.copy()\n",
    "df_volumret = df_volum.copy()\n",
    "df_volatret = df_volat.copy()\n",
    "df_mrret = df_mrcap.copy()\n",
    "df_retur = ((df_retur - df_retur.shift())/df_retur.shift())\n",
    "df_mrret = ((df_mrret - df_mrret.shift())/df_mrret.shift())\n",
    "df_volumret = ((df_volumret - df_volumret.shift())/df_volumret.shift())\n",
    "df_volatret = ((df_volatret - df_volatret.shift())/df_volatret.shift())\n",
    "df_retur['year'] = df_price.index.year\n",
    "df_mrret['year'] = df_mrcap.index.year\n",
    "df_volumret['year'] = df_volumret.index.year\n",
    "df_volatret['year'] = df_volatret.index.year\n",
    "\n",
    "\n",
    "df_price.to_pickle('export/df_price.pkl')\n",
    "df_mrcap.to_pickle('export/df_mrcap.pkl')\n",
    "df_volum.to_pickle('export/df_volum.pkl')\n",
    "df_volat.to_pickle('export/df_volat.pkl')\n",
    "\n",
    "df_retur.to_pickle('export/df_retur.pkl')\n",
    "df_mrret.to_pickle('export/df_mrret.pkl')\n",
    "df_volumret.to_pickle('export/df_volumret.pkl')\n",
    "df_volatret.to_pickle('export/df_volatret.pkl')\n",
    "\n",
    "df_edges = pd.read_pickle('export/df_edges_all_length_Series_tmp.pkl')\n",
    "df_edges = df_edges[(df_edges.org1.isin(df_price.columns))&(df_edges.org2.isin(df_price.columns))]\n",
    "df_edges.to_pickle('export/df_edges_all_length_Series.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 %\t\t\t\t%\t\t%\t\t\r"
     ]
    }
   ],
   "source": [
    "# ### Processing CoinGecko data\n",
    "# # let pandas treat infinities as NaN\n",
    "# pd.options.mode.use_inf_as_na = True\n",
    "# ### Load data... \n",
    "# series = pd.read_pickle(f't_series_{now}_CG.pkl')\n",
    "# series['time'] = pd.to_datetime(series.date)\n",
    "# series.rename(columns={'Close**':'price'}, inplace=True)\n",
    "# series.rename(columns={'Market Cap':'market cap'}, inplace=True)\n",
    "# series.rename(columns={'Volume':'volume'}, inplace=True)\n",
    "# series.sort_values('time',inplace=True)\n",
    "# series.loc[series.Low<0,'Low'] = np.nan\n",
    "# series.loc[series.High<0,'High'] = np.nan\n",
    "\n",
    "# # Evaluate the volatility per day as (max-min)/(max+min) del prezzo nella giornata\n",
    "# series['volatility'] = np.log(series['High']/series['Low'])\n",
    "# series.reset_index(drop=True,inplace=True)\n",
    "# series_keep = series.groupby('sym').count().reset_index()\n",
    "# coin_grouped_series = series.groupby('sym')\n",
    "\n",
    "# # Initialize the dataframe that will contain all the crypto timeseries\n",
    "# df_price = series.drop_duplicates('date')[['date']].reset_index(drop=True).rename(columns={'date':'time'})\n",
    "# df_mrcap = df_price.copy()\n",
    "# df_volum = df_price.copy()\n",
    "# df_volat = df_price.copy()\n",
    "# # Initialize the stable index to merge with\n",
    "# df_price_index = df_price.copy()\n",
    "# df_mrcap_index = df_price.copy()\n",
    "# df_volum_index = df_price.copy()\n",
    "# df_volat_index = df_price.copy()\n",
    "\n",
    "# # Cicle over all the grouped cryptocurrency timeseries\n",
    "# i=0\n",
    "# for group1 in coin_grouped_series:\n",
    "#     df_price[group1[0]] = df_price_index.merge(coin_grouped_series.get_group(group1[0])[['price','time']],left_on='time',right_on='time',how='left')['price']\n",
    "#     df_mrcap[group1[0]] = df_mrcap_index.merge(coin_grouped_series.get_group(group1[0])[['market cap','time']],left_on='time',right_on='time', how='left')['market cap']\n",
    "#     df_volum[group1[0]] = df_volum_index.merge(coin_grouped_series.get_group(group1[0])[['volume','time']],left_on='time',right_on='time', how='left')['volume']\n",
    "#     df_volat[group1[0]] = df_volat_index.merge(coin_grouped_series.get_group(group1[0])[['volatility','time']],left_on='time',right_on='time', how='left')['volatility']\n",
    "#     i+=1\n",
    "#     print(round(i/len(coin_grouped_series)*100,2),'%\\t\\t',end='\\r')\n",
    "\n",
    "# ###### Filtering cryptos with volume > 1e5 (already removed in notebook 1)\n",
    "# tmp_vol_filter = (df_volum.mean(axis=0)>1e5)\n",
    "# df_volum = df_volum.set_index('time')\n",
    "# df_volum = df_volum[[i for i in df_volum.columns if tmp_vol_filter.loc[i]==True]]\n",
    "# df_price = df_price.set_index('time')[df_volum.columns]\n",
    "# df_mrcap = df_mrcap.set_index('time')[df_volum.columns]\n",
    "# df_volat = df_volat.set_index('time')[df_volum.columns]\n",
    "\n",
    "# # If volume greater than 1e5 times the gratest capitalization (i.e. BTC maximum mrcap value) then remove value\n",
    "# df_volum = pd.DataFrame(data=np.where(df_volum<1e5*df_mrcap.max().max(),df_volum,np.NaN),columns=df_volum.columns,index=df_volum.index)\n",
    "# # If mrcap greater than the BTC max mrcap then remove value\n",
    "# df_mrcap = pd.DataFrame(data=np.where(df_mrcap<1.1*df_mrcap['BTC'].max(),df_mrcap,np.NaN),columns=df_mrcap.columns,index=df_mrcap.index)\n",
    "# # ######\n",
    "\n",
    "# df_price['year'] = df_price.index.year\n",
    "# df_mrcap['year'] = df_mrcap.index.year\n",
    "# df_volum['year'] = df_volum.index.year\n",
    "# df_volat['year'] = df_volat.index.year\n",
    "# df_retur = df_price.copy()\n",
    "# df_mrret = df_mrcap.copy()\n",
    "# df_volumret = df_volum.copy()\n",
    "# df_volatret = df_volat.copy()\n",
    "# df_mrret = df_mrcap.copy()\n",
    "# df_retur = ((df_retur - df_retur.shift())/df_retur.shift())\n",
    "# df_mrret = ((df_mrret - df_mrret.shift())/df_mrret.shift())\n",
    "# df_volumret = ((df_volumret - df_volumret.shift())/df_volumret.shift())\n",
    "# df_volatret = ((df_volatret - df_volatret.shift())/df_volatret.shift())\n",
    "# df_retur['year'] = df_price.index.year\n",
    "# df_mrret['year'] = df_mrcap.index.year\n",
    "# df_volumret['year'] = df_volumret.index.year\n",
    "# df_volatret['year'] = df_volatret.index.year\n",
    "\n",
    "\n",
    "# df_price.to_pickle('export/df_price_CG.pkl')\n",
    "# df_mrcap.to_pickle('export/df_mrcap_CG.pkl')\n",
    "# df_volum.to_pickle('export/df_volum_CG.pkl')\n",
    "# df_volat.to_pickle('export/df_volat_CG.pkl')\n",
    "\n",
    "# df_retur.to_pickle('export/df_retur_CG.pkl')\n",
    "# df_mrret.to_pickle('export/df_mrret_CG.pkl')\n",
    "# df_volumret.to_pickle('export/df_volumret_CG.pkl')\n",
    "# df_volatret.to_pickle('export/df_volatret_CG.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Select collaborative cryptos\n",
    "df_edges = pd.read_pickle('export/df_edges_all_length_Series.pkl')\n",
    "df_price_all = pd.read_pickle('export/df_price.pkl')\n",
    "df_mrcap_all = pd.read_pickle('export/df_mrcap.pkl')\n",
    "df_trvol_all = pd.read_pickle('export/df_volum.pkl')\n",
    "\n",
    "### Sort df_edges columns \"org1\" and \"org2\" based on different option values org1 -> active org2 -> passive\n",
    "### Possible OPTIONS: coin_age, mr_cap, tr_volum, first_action, coll_begins\n",
    "# ungroup_by = 'coin_age'\n",
    "# df_edges = df_edges.apply(lambda x: decoupling_df_edges(x,ungroup_by),axis=1)\n",
    "\n",
    "length = df_price_all.count().reset_index().rename(columns={'index':'org1',0:'len1'})\n",
    "length = length[length.org1!='year']\n",
    "df_edges = df_edges.merge(length, left_on='org1',right_on='org1')\n",
    "length = df_price_all.count().reset_index().rename(columns={'index':'org2',0:'len2'})\n",
    "df_edges = df_edges.merge(length, left_on='org2',right_on='org2')\n",
    "df_edges['coin_age1'] = [df_price_all[df_edges.org1[i]][df_price_all[df_edges.org1[i]].notna()].index[0] for i in range(len(df_edges))]\n",
    "df_edges['coin_age2'] = [df_price_all[df_edges.org2[i]][df_price_all[df_edges.org2[i]].notna()].index[0] for i in range(len(df_edges))]\n",
    "df_edges['mean_mr_share1'] = df_mrcap_all[df_edges.org1].divide(df_mrcap_all.sum(axis=1),axis=0).mean().values\n",
    "df_edges['mean_mr_share2'] = df_mrcap_all[df_edges.org2].divide(df_mrcap_all.sum(axis=1),axis=0).mean().values\n",
    "df_edges['mean_tr_volum1'] = df_trvol_all[df_edges.org1].divide(df_trvol_all.sum(axis=1),axis=0).mean().values\n",
    "df_edges['mean_tr_volum2'] = df_trvol_all[df_edges.org2].divide(df_trvol_all.sum(axis=1),axis=0).mean().values\n",
    "\n",
    "### Average market share and volume during the connection \n",
    "### 'mean_width' is the size in days of the connection\n",
    "mean_width = 7\n",
    "mr_cap_ecology = df_mrcap_all.loc[df_edges.coll_begins[1]-pd.Timedelta(mean_width,'D'):df_edges.coll_begins[1]+pd.Timedelta(mean_width,'D')].mean().sum()\n",
    "tr_vol_ecology = df_trvol_all.loc[df_edges.coll_begins[1]-pd.Timedelta(mean_width,'D'):df_edges.coll_begins[1]+pd.Timedelta(mean_width,'D')].mean().sum()\n",
    "df_edges['mr_share1'] = [df_mrcap_all[df_edges.org1[i]].loc[df_edges.coll_begins[i]-pd.Timedelta(mean_width,'D'):df_edges.coll_begins[i]+pd.Timedelta(mean_width,'D')].mean()/mr_cap_ecology for i in range(len(df_edges))]\n",
    "df_edges['mr_share2'] = [df_mrcap_all[df_edges.org2[i]].loc[df_edges.coll_begins[i]-pd.Timedelta(mean_width,'D'):df_edges.coll_begins[i]+pd.Timedelta(mean_width,'D')].mean()/mr_cap_ecology for i in range(len(df_edges))]\n",
    "df_edges['tr_volum1'] = [df_trvol_all[df_edges.org1[i]].loc[df_edges.coll_begins[i]-pd.Timedelta(mean_width,'D'):df_edges.coll_begins[i]+pd.Timedelta(mean_width,'D')].mean()/tr_vol_ecology for i in range(len(df_edges))]\n",
    "df_edges['tr_volum2'] = [df_trvol_all[df_edges.org2[i]].loc[df_edges.coll_begins[i]-pd.Timedelta(mean_width,'D'):df_edges.coll_begins[i]+pd.Timedelta(mean_width,'D')].mean()/tr_vol_ecology for i in range(len(df_edges))]\n",
    "df_edges.fillna(0,inplace=True)\n",
    "df_edges.to_pickle('export/df_edges_all_length_Series.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i1</th>\n",
       "      <th>org1</th>\n",
       "      <th>org2</th>\n",
       "      <th>events1</th>\n",
       "      <th>events2</th>\n",
       "      <th>actors</th>\n",
       "      <th>org1_begins</th>\n",
       "      <th>org2_begins</th>\n",
       "      <th>weights</th>\n",
       "      <th>coll_begins</th>\n",
       "      <th>...</th>\n",
       "      <th>coin_age1</th>\n",
       "      <th>coin_age2</th>\n",
       "      <th>mean_mr_share1</th>\n",
       "      <th>mean_mr_share2</th>\n",
       "      <th>mean_tr_volum1</th>\n",
       "      <th>mean_tr_volum2</th>\n",
       "      <th>mr_share1</th>\n",
       "      <th>mr_share2</th>\n",
       "      <th>tr_volum1</th>\n",
       "      <th>tr_volum2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>ADA</td>\n",
       "      <td>DCR</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>jcmincke</td>\n",
       "      <td>2017-12-12</td>\n",
       "      <td>2017-09-21</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>2017-12-12</td>\n",
       "      <td>...</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>2016-02-10</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.899730</td>\n",
       "      <td>0.068825</td>\n",
       "      <td>0.257254</td>\n",
       "      <td>0.125234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>BTC</td>\n",
       "      <td>DCR</td>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>jrick</td>\n",
       "      <td>2014-09-01</td>\n",
       "      <td>2016-02-09</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>2016-02-09</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-04-28</td>\n",
       "      <td>2016-02-10</td>\n",
       "      <td>0.741392</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>0.598979</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.869272</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.642754</td>\n",
       "      <td>0.000738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>AE</td>\n",
       "      <td>BTC</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>zack-bitcoin</td>\n",
       "      <td>2016-11-30</td>\n",
       "      <td>2014-04-20</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>2016-11-30</td>\n",
       "      <td>...</td>\n",
       "      <td>2017-11-08</td>\n",
       "      <td>2013-04-28</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.741392</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.598979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.786991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.689184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>AMB</td>\n",
       "      <td>BTC</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>robertsdotpm</td>\n",
       "      <td>2017-11-26</td>\n",
       "      <td>2014-04-17</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>2017-11-26</td>\n",
       "      <td>...</td>\n",
       "      <td>2017-10-28</td>\n",
       "      <td>2013-04-28</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.741392</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.598979</td>\n",
       "      <td>0.004397</td>\n",
       "      <td>24.019760</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>25.179334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>BCH</td>\n",
       "      <td>BTC</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>justusranvier</td>\n",
       "      <td>2017-10-27</td>\n",
       "      <td>2013-06-05</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>2017-10-27</td>\n",
       "      <td>...</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>2013-04-28</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.741392</td>\n",
       "      <td>0.051801</td>\n",
       "      <td>0.598979</td>\n",
       "      <td>1.033901</td>\n",
       "      <td>15.517184</td>\n",
       "      <td>2.899567</td>\n",
       "      <td>18.843176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>166</td>\n",
       "      <td>NEO</td>\n",
       "      <td>ONT</td>\n",
       "      <td>174</td>\n",
       "      <td>1</td>\n",
       "      <td>Celia18305</td>\n",
       "      <td>2017-10-23</td>\n",
       "      <td>2019-02-21</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>2019-02-21</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-09-09</td>\n",
       "      <td>2018-03-22</td>\n",
       "      <td>0.005631</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>0.009368</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>0.085009</td>\n",
       "      <td>0.070771</td>\n",
       "      <td>0.221416</td>\n",
       "      <td>0.057824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>183</td>\n",
       "      <td>PART</td>\n",
       "      <td>TIX</td>\n",
       "      <td>128</td>\n",
       "      <td>30</td>\n",
       "      <td>rynomster</td>\n",
       "      <td>2017-04-18</td>\n",
       "      <td>2017-01-25</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>2017-04-18</td>\n",
       "      <td>...</td>\n",
       "      <td>2017-09-16</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>197</td>\n",
       "      <td>PPC</td>\n",
       "      <td>XPM</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>sunnyking</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>2018-07-09</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>2018-07-09</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-07-07</td>\n",
       "      <td>2014-11-23</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.004530</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.001488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>226</td>\n",
       "      <td>SNT</td>\n",
       "      <td>WPR</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>Janther</td>\n",
       "      <td>2017-06-11</td>\n",
       "      <td>2017-07-13</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>2017-07-13</td>\n",
       "      <td>...</td>\n",
       "      <td>2017-07-29</td>\n",
       "      <td>2018-02-13</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>291</td>\n",
       "      <td>XZC</td>\n",
       "      <td>ZEN</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>josephnicholas</td>\n",
       "      <td>2018-07-14</td>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>2018-07-14</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-10-14</td>\n",
       "      <td>2017-06-14</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.013095</td>\n",
       "      <td>0.014864</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.059566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      i1  org1 org2  events1  events2          actors org1_begins org2_begins  \\\n",
       "0      2   ADA  DCR       12        1        jcmincke  2017-12-12  2017-09-21   \n",
       "1     31   BTC  DCR        1      359           jrick  2014-09-01  2016-02-09   \n",
       "2      5    AE  BTC       99        1    zack-bitcoin  2016-11-30  2014-04-20   \n",
       "3      8   AMB  BTC       13        4    robertsdotpm  2017-11-26  2014-04-17   \n",
       "4     18   BCH  BTC        1        7   justusranvier  2017-10-27  2013-06-05   \n",
       "..   ...   ...  ...      ...      ...             ...         ...         ...   \n",
       "202  166   NEO  ONT      174        1      Celia18305  2017-10-23  2019-02-21   \n",
       "203  183  PART  TIX      128       30       rynomster  2017-04-18  2017-01-25   \n",
       "204  197   PPC  XPM        4        7       sunnyking  2017-02-28  2018-07-09   \n",
       "205  226   SNT  WPR        1       15         Janther  2017-06-11  2017-07-13   \n",
       "206  291   XZC  ZEN        8       23  josephnicholas  2018-07-14  2018-03-11   \n",
       "\n",
       "      weights coll_begins  ...  coin_age1  coin_age2 mean_mr_share1  \\\n",
       "0    0.003268  2017-12-12  ... 2017-10-18 2016-02-10       0.012780   \n",
       "1    0.016340  2016-02-09  ... 2013-04-28 2016-02-10       0.741392   \n",
       "2    0.003268  2016-11-30  ... 2017-11-08 2013-04-28       0.000975   \n",
       "3    0.003268  2017-11-26  ... 2017-10-28 2013-04-28       0.000142   \n",
       "4    0.003268  2017-10-27  ... 2017-08-02 2013-04-28       0.040076   \n",
       "..        ...         ...  ...        ...        ...            ...   \n",
       "202  0.003268  2019-02-21  ... 2016-09-09 2018-03-22       0.005631   \n",
       "203  0.003268  2017-04-18  ... 2017-09-16 2017-12-08       0.000206   \n",
       "204  0.003268  2018-07-09  ... 2014-07-07 2014-11-23       0.001762   \n",
       "205  0.003268  2017-07-13  ... 2017-07-29 2018-02-13       0.000757   \n",
       "206  0.003268  2018-07-14  ... 2016-10-14 2017-06-14       0.000285   \n",
       "\n",
       "    mean_mr_share2 mean_tr_volum1 mean_tr_volum2 mr_share1  mr_share2  \\\n",
       "0         0.001062       0.000519       0.001216  0.899730   0.068825   \n",
       "1         0.001062       0.598979       0.001216  0.869272   0.000017   \n",
       "2         0.741392       0.000203       0.598979  0.000000   1.786991   \n",
       "3         0.741392       0.000071       0.598979  0.004397  24.019760   \n",
       "4         0.741392       0.051801       0.598979  1.033901  15.517184   \n",
       "..             ...            ...            ...       ...        ...   \n",
       "202       0.003337       0.009368       0.001633  0.085009   0.070771   \n",
       "203       0.000034       0.000235       0.000074  0.000000   0.000000   \n",
       "204       0.000205       0.000614       0.000116  0.005332   0.004530   \n",
       "205       0.000061       0.000555       0.000140  0.000000   0.000000   \n",
       "206       0.000243       0.000518       0.000480  0.013095   0.014864   \n",
       "\n",
       "    tr_volum1  tr_volum2  \n",
       "0    0.257254   0.125234  \n",
       "1    0.642754   0.000738  \n",
       "2    0.000000   0.689184  \n",
       "3    0.003965  25.179334  \n",
       "4    2.899567  18.843176  \n",
       "..        ...        ...  \n",
       "202  0.221416   0.057824  \n",
       "203  0.000000   0.000000  \n",
       "204  0.003639   0.001488  \n",
       "205  0.000000   0.000000  \n",
       "206  0.019274   0.059566  \n",
       "\n",
       "[207 rows x 32 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random edges generation -> generating pairs for 3 different baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Edges generator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "measure_serie = 'retur'\n",
    "### Generates 3 datasets with edges:\n",
    "df_measure = pd.read_pickle('export/df_{}.pkl'.format(measure_serie))\n",
    "df_measure = df_measure.drop('year',axis=1)\n",
    "conn_couples = pd.read_pickle('export/df_edges_all_length_Series.pkl')\n",
    "conn_couples = conn_couples[conn_couples.coll_begins>df_measure.first_valid_index()]\n",
    "### 1- connected/collaborative couples ('conn_couples.pkl')\n",
    "conn_couples.to_pickle('conn_couples.pkl')\n",
    "conn_couples.to_pickle('conn_couples_safe.pkl')\n",
    "\n",
    "### 2- N random couples with connection time distributed as the connection_time distribution ('rand_conn_time_couples.pkl')\n",
    "# Select the desired time period from market data\n",
    "N = 1e5\n",
    "Rcouples = conn_couples[['org1','org2','coll_begins']].sample(int(N),replace=True).reset_index(drop=True)\n",
    "Rcouples = Rcouples.rename(columns={'org1':'original_org1','org2':'original_org2'})\n",
    "Rcouples['org1'],Rcouples['org2'] = np.transpose([list(np.random.choice(df_measure.columns,2,replace=False)) for i in range(int(N))])\n",
    "Rcouples['coll_action_latter'] = conn_couples['coll_action_latter'].sample(int(N),replace=True).reset_index(drop=True)\n",
    "Rcouples = Rcouples[(Rcouples.org1!=Rcouples.original_org1)&(Rcouples.org1!=Rcouples.original_org2)&\\\n",
    "                    (Rcouples.org2!=Rcouples.original_org1)&(Rcouples.org2!=Rcouples.original_org2)].reset_index(drop=True)\n",
    "Rcouples.to_pickle('rand_conn_time_couples.pkl')\n",
    "Rcouples.to_pickle('rand_conn_time_couples_safe.pkl')\n",
    "\n",
    "\n",
    "### 3- N random coupling as in 2 plus age condition ('rand_conn_time_and_age_couples.pkl')\n",
    "###    age condition: sampled cryptos must have the same age of the coupled one (in a \"tolerance\" range)\n",
    "con_dates = conn_couples.coll_begins\n",
    "tolerance = 7 # difference in days from age to which accept  the other candidates\n",
    "### List the ages of the connected cryptos (for each row we have 2 ages)\n",
    "ages_latter = []\n",
    "ages_former = []\n",
    "crypto_latter = []\n",
    "crypto_former = []\n",
    "first_indexes = [df_measure[col].first_valid_index() for col in df_measure.columns]\n",
    "columns = list(df_measure.columns)\n",
    "v_index = pd.DataFrame(index=columns,data={'indexes':first_indexes})\n",
    "old = []\n",
    "new = []\n",
    "nold = []\n",
    "nnew = []\n",
    "nageold = []\n",
    "nagenew = []\n",
    "\n",
    "\n",
    "for i, couple in conn_couples.iterrows():\n",
    "    if couple.coin_age1>couple.coin_age2:\n",
    "        old_crypto = couple.org2\n",
    "        old_enters = couple.coin_age2\n",
    "        new_crypto = couple.org1\n",
    "        new_enters = couple.coin_age1\n",
    "    else:\n",
    "        old_crypto = couple.org1\n",
    "        old_enters = couple.coin_age1\n",
    "        new_crypto = couple.org2\n",
    "        new_enters = couple.coin_age2\n",
    "    nold.append(old_crypto)\n",
    "    nnew.append(new_crypto)\n",
    "    nageold.append(old_enters)\n",
    "    nagenew.append(new_enters)\n",
    "    org_market_entrance = v_index.drop([couple.org1,couple.org2])\n",
    "    tmp = org_market_entrance[(org_market_entrance.indexes<couple.coin_age1+pd.Timedelta(tolerance,'days'))&(org_market_entrance.indexes>couple.coin_age1-pd.Timedelta(tolerance,'days'))&(org_market_entrance.indexes<couple.coin_age2+pd.Timedelta(tolerance,'days'))&(org_market_entrance.indexes>couple.coin_age2-pd.Timedelta(tolerance,'days'))]\n",
    "    ## If a crypto is age-compatible with both old and new we randomly assign it to the old or new group\n",
    "    if len(tmp)>0:\n",
    "        old_new_num = round(np.random.uniform(0,len(tmp)))\n",
    "        old_rand = list(np.random.choice(list(tmp.index), old_new_num))\n",
    "        new_rand = [cpt for cpt in tmp.index if not cpt in old_rand]\n",
    "    else:\n",
    "        old_rand = []\n",
    "        new_rand = []\n",
    "    org_market_entrance = org_market_entrance.drop(tmp.index)\n",
    "    old_row = list(org_market_entrance[(org_market_entrance.indexes<couple.coin_age1+pd.Timedelta(tolerance,'days'))&(org_market_entrance.indexes>couple.coin_age1-pd.Timedelta(tolerance,'days'))].index)\n",
    "    new_row = list(org_market_entrance[(org_market_entrance.indexes<couple.coin_age2+pd.Timedelta(tolerance,'days'))&(org_market_entrance.indexes>couple.coin_age2-pd.Timedelta(tolerance,'days'))].index)\n",
    "    old_row = old_row+old_rand\n",
    "    new_row = new_row+new_rand\n",
    "    if (len(old_row)>0)&(len(new_row)>0):\n",
    "        old.append(old_row)\n",
    "        new.append(new_row)\n",
    "    else:\n",
    "        old.append([])\n",
    "        new.append([])\n",
    "\n",
    "## in this case org1 is also the oldest one (using the market entrance date)\n",
    "new_conn_couples = pd.DataFrame({'org_old':nold,'org_new':nnew,'age_old':nageold,'age_new':nagenew,'coll_begins':conn_couples.coll_begins,'coll_action_latter':conn_couples.coll_action_latter})\n",
    "org_old = []\n",
    "org_new = []\n",
    "age_old = []\n",
    "age_new = []\n",
    "coll_be = []\n",
    "coll_lat = []\n",
    "org1 = []\n",
    "org2 = []\n",
    "\n",
    "for i in range(len(old)):\n",
    "        s_old = old[i]\n",
    "        s_new = new[i]\n",
    "        s_con = conn_couples.iloc[i].coll_begins\n",
    "        s_col = conn_couples.iloc[i].coll_action_latter\n",
    "        if (len(s_old)>0)&(len(s_new)>0):\n",
    "            for old1, new1 in itertools.product(s_old,s_new):\n",
    "                if (old1!=new_conn_couples.iloc[i].org_old)&(new1!=new_conn_couples.iloc[i].org_new):\n",
    "                    org_old.append(old1)\n",
    "                    org_new.append(new1)\n",
    "                    coll_be.append(s_con)\n",
    "                    coll_lat.append(s_col)\n",
    "                    age_old.append(v_index.loc[old1][0])\n",
    "                    age_new.append(v_index.loc[new1][0])\n",
    "                    org1.append(new_conn_couples.iloc[i].org_old)\n",
    "                    org2.append(new_conn_couples.iloc[i].org_new)\n",
    "RATcouples = pd.DataFrame({'org1':org_old,'org2':org_new,'age_old':age_old,'age_new':age_new,'coll_begins':coll_be,'coll_action_latter':coll_lat,'original_org1':org1,'original_org2':org2})\n",
    "RATcouples = RATcouples.sample(int(N),replace=True).reset_index(drop=True)\n",
    "RATcouples = RATcouples[(RATcouples.org1!=RATcouples.original_org1)&(RATcouples.org1!=RATcouples.original_org2)&\\\n",
    "                    (RATcouples.org2!=RATcouples.original_org1)&(RATcouples.org2!=RATcouples.original_org2)].reset_index(drop=True)\n",
    "RATcouples.to_pickle('rand_conn_time_and_age_couples.pkl')\n",
    "RATcouples.to_pickle('rand_conn_time_and_age_couples_safe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
